{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LM & peot classificatin.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zc0RcRv1UbE"
      },
      "source": [
        "**Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ka1xc4Ip1ir0"
      },
      "source": [
        "!pip install hazm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkZbgtDH1VlA"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from hazm import word_tokenize\n",
        "from heapq import nlargest"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "979hxCXHHbTy"
      },
      "source": [
        "# Section 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Si6mrj3Yf8aj"
      },
      "source": [
        "**Tokenizer functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eedgb37y1wiX"
      },
      "source": [
        "def unigram_line(line):\n",
        "  return [word for word in word_tokenize(line)]\n",
        "\n",
        "def bigram_line(line):\n",
        "  bi=[]\n",
        "  text = [word for word in word_tokenize(line)]\n",
        "  for i in range(len(text)-1):\n",
        "      bi.append(text[i] + ' ' + text[i+1])\n",
        "  return bi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jmHWrT_2ILm"
      },
      "source": [
        "**Preparing Data**\n",
        "\n",
        "this function make a dictionary that contain all we need from data in following sections"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPxnsKn62b9I"
      },
      "source": [
        "def prepare_data(add):\n",
        "  data= {'all': {'bigrams' : [] , 'unigrams' : [] ,'beyt' : [] , 'bigram_count':{} , 'unigram_count':{} }}\n",
        "  poets = []\n",
        "  with open( add ,  encoding = 'UTF-8') as file:\n",
        "    data_lines = file.readlines()\n",
        "  for i in range(0,len(data_lines) , 3):\n",
        "    if data_lines[i][:-1] not in poets: \n",
        "      poets.append(data_lines[i][:-1])\n",
        "      data[data_lines[i][:-1]] = {'bigrams' : [] , 'unigrams' : [] ,'beyt' : [] , \n",
        "                                  'bigram_count':{} , 'unigram_count':{} }\n",
        "    data[data_lines[i][:-1]]['beyt'] += [[data_lines[i+1][:-1] , data_lines[i+2][:-1]]]\n",
        "    data['all']['beyt'] += [[data_lines[i+1][:-1] , data_lines[i+2][:-1]]]\n",
        "\n",
        "    tokens =[]\n",
        "    tokens += word_tokenize(data_lines[i+1][:-1])\n",
        "    tokens += word_tokenize(data_lines[i+2][:-1])\n",
        "    tokens_set = list(set(tokens))\n",
        "    for w in tokens:\n",
        "      if w in data['all']['unigram_count']:\n",
        "        data['all']['unigram_count'][w] +=1\n",
        "      else :\n",
        "        data['all']['unigram_count'][w] =1\n",
        "\n",
        "      if w in data[data_lines[i][:-1]]['unigram_count']:\n",
        "        data[data_lines[i][:-1]]['unigram_count'][w] +=1\n",
        "      else: \n",
        "        data[data_lines[i][:-1]]['unigram_count'][w] =1\n",
        "\n",
        "    data[data_lines[i][:-1]]['unigrams'] += tokens\n",
        "    data['all']['unigrams'] += tokens\n",
        "\n",
        "    for mes in range(2):\n",
        "      tokens =[]\n",
        "      tokens += word_tokenize(data_lines[i+1+mes][:-1])\n",
        "      for idx in range(len(tokens)-1):\n",
        "        new_bi = tokens[idx] + ' ' + tokens[idx+1]\n",
        "        if new_bi in data['all']['bigram_count']:\n",
        "          data['all']['bigram_count'][new_bi] +=1\n",
        "        else :\n",
        "          data['all']['bigram_count'][new_bi] =1\n",
        "\n",
        "        if new_bi in data[data_lines[i][:-1]]['bigram_count']:\n",
        "          data[data_lines[i][:-1]]['bigram_count'][new_bi] +=1\n",
        "        else: \n",
        "          data[data_lines[i][:-1]]['bigram_count'][new_bi] =1\n",
        "\n",
        "        data[data_lines[i][:-1]]['bigrams'].append(new_bi)\n",
        "        data['all']['bigrams'].append(new_bi)\n",
        "  return data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_-ZOLIeBDqx"
      },
      "source": [
        "reading train , validation and test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aebtqm4lBI4R",
        "outputId": "49c0fd34-25c4-4acd-fbf1-3c5fe375ad1e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "train_data = prepare_data('/content/drive/MyDrive/nlp/HW1/train.txt')\n",
        "val_data = prepare_data('/content/drive/MyDrive/nlp/HW1/valid.txt')\n",
        "test_data = prepare_data ('/content/drive/MyDrive/nlp/HW1/test.txt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOh2_b25ArR6",
        "outputId": "dbea549e-5a75-4868-8aac-21d69bb31612"
      },
      "source": [
        "print(train_data['all']['bigram_count']['ای دل'])\n",
        "print(train_data['bahar']['bigram_count']['ای دل'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "187\n",
            "26\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCy0kHoTHHy0"
      },
      "source": [
        "# Section 2\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6i61qEqkCteu"
      },
      "source": [
        "**A**\n",
        "\n",
        "unigram language model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thUX2G_VFZWI"
      },
      "source": [
        "all_vocab = len(train_data['all']['unigrams'])             ###### number of all words in train data\n",
        "vocab_size = len(set(train_data['all']['unigrams']))       ###### number of unique words in train data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLKwwikTFfvO",
        "outputId": "37457090-227f-4259-aa1f-20b4fb0b5741"
      },
      "source": [
        "def Simple_Unigram_Perplexity(beyt):\n",
        "  per_list=[]\n",
        "  prob = 1\n",
        "  tok=[]\n",
        "  tok += word_tokenize(beyt[0])\n",
        "  tok += word_tokenize(beyt[1])\n",
        "  for t in tok:\n",
        "    try :\n",
        "      C_unigram = train_data['all']['unigram_count'][t]  \n",
        "      prob *= (C_unigram  / all_vocab ) \n",
        "    except :\n",
        "      prob *= ( 1  / vocab_size ) \n",
        "\n",
        "  len_beyt = len(tok)\n",
        "  perplexity = prob ** (-1 / len_beyt)\n",
        "\n",
        "  return perplexity\n",
        "\n",
        "simple_unigram_perplexity_list=[]\n",
        "for w in test_data['all']['beyt']:\n",
        "  simple_unigram_perplexity_list.append(Simple_Unigram_Perplexity(w))\n",
        "    \n",
        "np.mean(simple_unigram_perplexity_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2694.8088633530874"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaQLiXIKgR_I"
      },
      "source": [
        "Unigram Language Model with Absolute Discounting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tl2hUvl9CYEW",
        "outputId": "0b7e9e0b-5fbb-4337-8f2a-949821709ee6"
      },
      "source": [
        "def Unigram_Perplexity(beyt , delta = 0.5):\n",
        "  per_list=[]\n",
        "  prob = 1\n",
        "  tok=[]\n",
        "  tok += word_tokenize(beyt[0])\n",
        "  tok += word_tokenize(beyt[1])\n",
        "  B = 1\n",
        "  for t in tok:\n",
        "    try :\n",
        "      train_data['all']['unigram_count'][t]\n",
        "      B += 1\n",
        "    except : pass\n",
        "  for t in tok:\n",
        "      try :\n",
        "        C_unigram = train_data['all']['unigram_count'][t]\n",
        "        prob *= (max((C_unigram - delta) , 0) / all_vocab ) \n",
        "      except :\n",
        "        prob *= ((delta * B ) / ( len(tok) * vocab_size))\n",
        "\n",
        "  len_beyt = len(tok)\n",
        "  perplexity = prob ** (-1 / len_beyt)\n",
        "  return perplexity\n",
        "\n",
        "unigram_perplexity_list=[]\n",
        "for w in val_data['all']['beyt']:\n",
        "  unigram_perplexity_list.append(Unigram_Perplexity(w , delta= 0.8))\n",
        "    \n",
        "np.mean(unigram_perplexity_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3008.4601860638077"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MK6VWxqEglSr"
      },
      "source": [
        "finding best delta for unigram model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "G4w4GrfeDnnj",
        "outputId": "c3761c4b-a955-40f8-ace0-517f2e0b78d0"
      },
      "source": [
        "delta_list = list(np.arange(0.02 ,1 , 0.02))\n",
        "delta_per=[]\n",
        "for d in delta_list : \n",
        "    unigram_perplexity_list=[]\n",
        "    for w in val_data['all']['beyt']:\n",
        "        unigram_perplexity_list.append(Unigram_Perplexity(w , delta = d))\n",
        "    delta_per.append(np.mean(unigram_perplexity_list))\n",
        "\n",
        "print(f' best delta for unigram model is : {delta_list[np.argmin(delta_per)]} ')\n",
        "print('   ')\n",
        "print('   ')\n",
        "print('   ')\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(delta_list , delta_per)\n",
        "ax.set(xlabel='Delta', ylabel='Perplexity',\n",
        "       title='Optimum delta in Unigram Language Model')\n",
        "ax.grid()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " best delta for unigram model is : 0.54 \n",
            "   \n",
            "   \n",
            "   \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qi65duUDVRK"
      },
      "source": [
        "Bigram Language Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKv_NJ0MDaVJ",
        "outputId": "fc018cd6-43ff-44ab-f0c5-fe85dd404383"
      },
      "source": [
        "def Bigram_Perplexity(beyt , delta):\n",
        "  per_list=[]\n",
        "  tok = []\n",
        "  tok += bigram_line(beyt[0])\n",
        "  tok += bigram_line(beyt[1])  \n",
        "  prob = 1\n",
        "  B = 1\n",
        "  for t in tok:\n",
        "    try :\n",
        "      train_data['all']['bigram_count'][t]\n",
        "      B += 1\n",
        "    except : pass\n",
        "  for t in tok:\n",
        "    token = unigram_line(t)\n",
        "    try: \n",
        "      c_bi = train_data['all']['bigram_count'][t]\n",
        "      c_token = train_data['all']['unigram_count'][token[0]]\n",
        "      prob *= (max((c_bi - delta) , 0) / c_token)  + ((delta * B * c_unigram) / (c_token * all_vocab))\n",
        "\n",
        "    except : \n",
        "      try : \n",
        "        c_unigram = train_data['all']['unigram_count'][token[1]]\n",
        "        c_token = 1\n",
        "        prob *=  ((delta * B * c_unigram) / (c_token * all_vocab))\n",
        "      except :\n",
        "        prob *= 1 / vocab_size\n",
        "              \n",
        "  len_beyt = len(tok)\n",
        "  perplexity = prob ** (-1 / (len_beyt))\n",
        "\n",
        "  return perplexity\n",
        "\n",
        "\n",
        "bigram_perplexity_list=[]\n",
        "\n",
        "for w in val_data['all']['beyt']:\n",
        "    bigram_perplexity_list.append(bi_perplexity(w , delta = 0.5))\n",
        "    \n",
        "np.mean(bigram_perplexity_list)\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1419.6438094799328"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yo-dI0oFg5dV"
      },
      "source": [
        "finding best delta for bigram language model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "MzuEELrkMrdg",
        "outputId": "6eddfcf7-872a-4c63-d24e-e30371b3f68f"
      },
      "source": [
        "delta_list = list(np.arange(0.4 ,2.4 , 0.04))\n",
        "delta_per_bi=[]\n",
        "for d in delta_list : \n",
        "    bigram_perplexity_list=[]\n",
        "    for w in val_data['all']['beyt']:\n",
        "        bigram_perplexity_list.append(Bigram_Perplexity(w , delta = d))\n",
        "    delta_per_bi.append(np.mean(bigram_perplexity_list))\n",
        "\n",
        "print(f' best delta for bigram model is : {delta_list[np.argmin(delta_per_bi)]} ')\n",
        "print('   ')\n",
        "print('   ')\n",
        "print('   ')\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(delta_list , delta_per_bi)\n",
        "ax.set(xlabel='Delta', ylabel='Perplexity',\n",
        "       title='Optimum delta in Bigram Language Model')\n",
        "ax.grid()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " best delta for bigram model is : 0.8799999999999998 \n",
            "   \n",
            "   \n",
            "   \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEhxYYPahRjc"
      },
      "source": [
        "Perplexity with optimum delta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0sKeVULOPoY",
        "outputId": "964aee61-6f76-42e1-ca92-a12aa00ab2b8"
      },
      "source": [
        "unigram_perplexity_list=[]\n",
        "for w in val_data['all']['beyt']:\n",
        "  unigram_perplexity_list.append(Unigram_Perplexity(w , delta= 0.54))\n",
        "    \n",
        "print(f'Perplexity of Unigram model : {np.mean(unigram_perplexity_list)}')\n",
        "print('    ')\n",
        "\n",
        "bigram_perplexity_list=[]\n",
        "for w in val_data['all']['beyt']:\n",
        "    bigram_perplexity_list.append(bi_perplexity(w , delta = 0.88))\n",
        "\n",
        "print(f'Perplexity of Bigram model : {np.mean(bigram_perplexity_list)}')\n",
        "print('    ')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Perplexity of Unigram model : 2963.503007977819\n",
            "    \n",
            "Perplexity of Bigram model : 1128.9539521654326\n",
            "    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKzCJFA7LXkl"
      },
      "source": [
        "## Section  2-B"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceKRjNlxLmJP"
      },
      "source": [
        "def prepare2_data(add , labels):\n",
        "    data= {'all': {'bigrams' : [] , 'unigrams' : [] ,'beyt' : [] , 'bigram_count':{} , 'unigram_count':{} }}\n",
        "    poets = []\n",
        "    with open( add ,  encoding = 'UTF-8') as file:\n",
        "        data_lines = file.readlines()\n",
        "    for i in range(0,len(data_lines) , 3):\n",
        "        if (data_lines[i][:-1] not in poets) and (data_lines[i][:-1] in labels) : \n",
        "            poets.append(data_lines[i][:-1])\n",
        "            data[data_lines[i][:-1]] = {'bigrams' : [] , 'unigrams' : [] ,'beyt' : [] , \n",
        "                                        'bigram_count':{} , 'unigram_count':{} }\n",
        "        if data_lines[i][:-1] in labels :\n",
        "            data[data_lines[i][:-1]]['beyt'] += [[data_lines[i+1][:-1] , data_lines[i+2][:-1]]]\n",
        "            data['all']['beyt'] += [[data_lines[i+1][:-1] , data_lines[i+2][:-1]]]\n",
        "\n",
        "            tokens =[]\n",
        "            tokens += word_tokenize(data_lines[i+1][:-1])\n",
        "            tokens += word_tokenize(data_lines[i+2][:-1])\n",
        "            tokens_set = list(set(tokens))\n",
        "            for w in tokens_set:\n",
        "                if w in data['all']['unigram_count']:\n",
        "                    data['all']['unigram_count'][w] +=1\n",
        "                else :\n",
        "                    data['all']['unigram_count'][w] =1\n",
        "\n",
        "                if w in data[data_lines[i][:-1]]['unigram_count']:\n",
        "                    data[data_lines[i][:-1]]['unigram_count'][w] +=1\n",
        "                else: \n",
        "                    data[data_lines[i][:-1]]['unigram_count'][w] =1\n",
        "\n",
        "            data[data_lines[i][:-1]]['unigrams'] += tokens\n",
        "            data['all']['unigrams'] += tokens\n",
        "\n",
        "            for mes in range(2):\n",
        "                tokens =[]\n",
        "                tokens += word_tokenize(data_lines[i+1+mes][:-1])\n",
        "                for idx in range(len(tokens)-1):\n",
        "                    new_bi = tokens[idx] + ' ' + tokens[idx+1]\n",
        "                    if new_bi in data['all']['bigram_count']:\n",
        "                        data['all']['bigram_count'][new_bi] +=1\n",
        "                    else :\n",
        "                        data['all']['bigram_count'][new_bi] =1\n",
        "\n",
        "                    if new_bi in data[data_lines[i][:-1]]['bigram_count']:\n",
        "                        data[data_lines[i][:-1]]['bigram_count'][new_bi] +=1\n",
        "                    else: \n",
        "                        data[data_lines[i][:-1]]['bigram_count'][new_bi] =1\n",
        "\n",
        "                    data[data_lines[i][:-1]]['bigrams'].append(new_bi)\n",
        "                    data['all']['bigrams'].append(new_bi)\n",
        "    return data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOtA1cPlhgaw"
      },
      "source": [
        "perplexity for each poet in unigram model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0Ax29rnLdIu",
        "outputId": "3bede384-3120-42fe-f9df-007c28d4ae95"
      },
      "source": [
        "def Unigram_Perplexity(beyt , delta = 0.5):\n",
        "  per_list=[]\n",
        "  prob = 1\n",
        "  tok=[]\n",
        "  tok += word_tokenize(beyt[0])\n",
        "  tok += word_tokenize(beyt[1])\n",
        "  B = 1\n",
        "  for t in tok:\n",
        "    try :\n",
        "      train2_data['all']['unigram_count'][t]\n",
        "      B += 1\n",
        "    except : pass\n",
        "  for t in tok:\n",
        "      try :\n",
        "        C_unigram = train2_data['all']['unigram_count'][t]\n",
        "        prob *= (max((C_unigram - delta) , 1) / all_vocab ) \n",
        "      except :\n",
        "        #prob *= ((delta * B ) / ( len(tok) * vocab_size))\n",
        "        prob *= ( 1 /  vocab_size ) \n",
        "\n",
        "  len_beyt = len(tok)\n",
        "  perplexity = prob ** (-1 / (len_beyt * 1.25) )\n",
        "  return perplexity\n",
        "\n",
        "labels = ['moulavi' , 'amir' , 'sanaee' , 'ghaani' , 'bahar' , 'khosro']\n",
        "\n",
        "for l in labels:\n",
        "  train2_data = prepare2_data('/content/drive/MyDrive/nlp/HW1/train.txt' , [l])\n",
        "  test2_data = prepare2_data('/content/drive/MyDrive/nlp/HW1/test.txt' , [l])\n",
        "  unigram_perplexity_list=[]\n",
        "  for w in test2_data['all']['beyt']:\n",
        "    unigram_perplexity_list.append(Unigram_Perplexity(w , delta= 0.5)) \n",
        "  a = np.mean(unigram_perplexity_list)\n",
        "  print(f'Perplexity of {l} is :' + '{:f}'.format(a))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Perplexity of moulavi is :1598.035806\n",
            "Perplexity of amir is :1441.837406\n",
            "Perplexity of sanaee is :1652.075321\n",
            "Perplexity of ghaani is :2143.182654\n",
            "Perplexity of bahar is :2504.933071\n",
            "Perplexity of khosro is :1354.065493\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjqkHZ73htTN"
      },
      "source": [
        "perplexity for each poet in bigram model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifOqiNBTXYK7",
        "outputId": "b7e03d95-a8fb-49e5-cdf2-c6637e66ed0a"
      },
      "source": [
        "def Bigram_Perplexity(beyt , delta):\n",
        "  per_list=[]\n",
        "  tok = []\n",
        "  tok += bigram_line(beyt[0])\n",
        "  tok += bigram_line(beyt[1])  \n",
        "  prob = 1\n",
        "  B = 1\n",
        "  for t in tok:\n",
        "    try :\n",
        "      train2_data['all']['bigram_count'][t]\n",
        "      B += 1\n",
        "    except : pass\n",
        "  for t in tok:\n",
        "    token = unigram_line(t)\n",
        "    try: \n",
        "      c_bi = train2_data['all']['bigram_count'][t]\n",
        "      c_token = train2_data['all']['unigram_count'][token[0]]\n",
        "      prob *= (max((c_bi - delta) , 0) / c_token)  + ((delta * B * c_unigram) / (c_token * all_vocab))\n",
        "\n",
        "    except : \n",
        "      try : \n",
        "        c_unigram = train2_data['all']['unigram_count'][token[1]]\n",
        "        c_token = 1\n",
        "        prob *=  ((delta * B * c_unigram) / (c_token * all_vocab))\n",
        "      except :\n",
        "        prob *= 1 / vocab_size\n",
        "              \n",
        "  len_beyt = len(tok)\n",
        "  perplexity = prob ** (-1 / (len_beyt ))\n",
        "\n",
        "  return perplexity\n",
        "\n",
        "\n",
        "for l in labels:\n",
        "  train2_data = prepare2_data('/content/drive/MyDrive/nlp/HW1/train.txt' , [l])\n",
        "  test2_data = prepare2_data('/content/drive/MyDrive/nlp/HW1/test.txt' , [l])\n",
        "  unigram_perplexity_list=[]\n",
        "  for w in test2_data['all']['beyt']:\n",
        "    unigram_perplexity_list.append(Bigram_Perplexity(w , delta= 0.5)) \n",
        "  a = np.mean(unigram_perplexity_list)\n",
        "  print(f'Perplexity of {l} is :' + '{:f}'.format(a))\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Perplexity of moulavi is :630.051689\n",
            "Perplexity of amir is :302.801629\n",
            "Perplexity of sanaee is :706.955372\n",
            "Perplexity of ghaani is :727.240675\n",
            "Perplexity of bahar is :1201.432556\n",
            "Perplexity of khosro is :425.424933\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4W0dcXJFG1x"
      },
      "source": [
        "\n",
        "# Section 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYcZT-OVHmsg"
      },
      "source": [
        "Function that return information gain of a word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3PTkyjuFY7C"
      },
      "source": [
        "def IG_calculator(w):\n",
        "  p_w = train_data['all']['unigram_count'][w] / len(train_data['all']['beyt'])\n",
        "  term1 ,term2 ,term3= 0 ,0 ,0\n",
        "  labels = list(train_data.keys())[1:]\n",
        "\n",
        "  for k in labels:\n",
        "    p_ci = len(train_data[k]['beyt']) / len(train_data['all']['beyt'])\n",
        "    term1 += p_ci * np.log2(p_ci)\n",
        "    try : \n",
        "        p_ci_given_w = train_data[k]['unigram_count'][w] / train_data['all']['unigram_count'][w]\n",
        "    except : \n",
        "        p_ci_given_w = 0\n",
        "    try : \n",
        "        p_ci_given_wnot = (len(train_data[k]['beyt']) - train_data[k]['unigram_count'][w]) / (len(train_data['all']['beyt'])-train_data['all']['unigram_count'][w])\n",
        "    except :\n",
        "        p_ci_given_wnot = p_ci_given_wnot = len(train_data[k]['beyt']) / (len(train_data['all']['beyt'])-train_data['all']['unigram_count'][w])\n",
        "    if p_ci_given_w == 0 : \n",
        "      term2 +=0\n",
        "      term3 += ( (1-p_w) * (p_ci_given_wnot) * np.log2(p_ci_given_wnot) )\n",
        "    else:\n",
        "        term2 += (p_w * p_ci_given_w * np.log2(p_ci_given_w))\n",
        "        term3 += ( (1-p_w) * (p_ci_given_wnot) * np.log2(p_ci_given_wnot) )\n",
        "  ig_w = -term1 + term2 + term3\n",
        "  return ig_w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcKwXvAsaCL0"
      },
      "source": [
        "train_data_unigram_set = list(set(train_data['all']['unigrams']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFb8IvQnh6lB"
      },
      "source": [
        "top 10 words with highest IG"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vVBEqU4aG0E",
        "outputId": "4de82832-682c-4c83-cd3f-c7fcfe48756b"
      },
      "source": [
        "ig_list=[]\n",
        "for w in train_data_unigram_set:\n",
        "    ig_list.append(IG_calculator(w))\n",
        "    \n",
        "lst = pd.Series(ig_list)\n",
        "ig_idx = lst.nlargest(200)\n",
        "ig_idx = list(ig_idx.index)\n",
        "ig_bestwords = {}\n",
        "for i in ig_idx:\n",
        "  ig_bestwords[train_data_unigram_set[i]] = ig_list[i]\n",
        "\n",
        "ig_bestwords\n",
        "nl = nlargest(10, ig_bestwords , key = ig_bestwords.get)\n",
        "for k in nl:\n",
        "  print(f'{k} : IG = {ig_bestwords[k]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "و : IG = 0.05804361575230521\n",
            "به : IG = 0.055387801097265266\n",
            "تو : IG = 0.04912675554820578\n",
            "می : IG = 0.03965111664820009\n",
            "خسرو : IG = 0.024797429563316875\n",
            "است : IG = 0.024021992172670537\n",
            "که : IG = 0.02315160869299926\n",
            "او : IG = 0.02247957657017796\n",
            "من : IG = 0.01716599072504632\n",
            "دل : IG = 0.013061929607952827\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJ6BbfgnaMjI",
        "outputId": "ab705945-05fd-4c84-d189-3bfa80e0a199"
      },
      "source": [
        "ig_best_by_poets={}\n",
        "labels = list(train_data.keys())[1:]\n",
        "for w in ig_bestwords:\n",
        "    max_l=[]\n",
        "    for l in labels:\n",
        "        try : max_l.append(train_data[l]['unigram_count'][w])\n",
        "        except : max_l.append(0)\n",
        "    ig_best_by_poets[w] = labels[np.argmax(max_l)]\n",
        "ig_best_by_poets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'آب': 'moulavi',\n",
              " 'آن': 'moulavi',\n",
              " 'آنچ': 'moulavi',\n",
              " 'آنکه': 'sanaee',\n",
              " 'ا': 'amir',\n",
              " 'ابر': 'ghaani',\n",
              " 'ار': 'khosro',\n",
              " 'از': 'sanaee',\n",
              " 'ازان': 'khosro',\n",
              " 'است': 'amir',\n",
              " 'اقبال': 'amir',\n",
              " 'اندر': 'moulavi',\n",
              " 'او': 'amir',\n",
              " 'اگر': 'khosro',\n",
              " 'ای': 'moulavi',\n",
              " 'ایران': 'bahar',\n",
              " 'ایزد': 'amir',\n",
              " 'این': 'moulavi',\n",
              " 'باد': 'amir',\n",
              " 'باز': 'khosro',\n",
              " 'باشد': 'sanaee',\n",
              " 'بانگ': 'moulavi',\n",
              " 'بحر': 'ghaani',\n",
              " 'بخت': 'amir',\n",
              " 'بر': 'sanaee',\n",
              " 'برای': 'sanaee',\n",
              " 'بزم': 'amir',\n",
              " 'بسکه': 'ghaani',\n",
              " 'به': 'ghaani',\n",
              " 'بهار': 'bahar',\n",
              " 'بود': 'moulavi',\n",
              " 'بی': 'moulavi',\n",
              " 'تا': 'moulavi',\n",
              " 'تاکه': 'amir',\n",
              " 'ترا': 'sanaee',\n",
              " 'تست': 'sanaee',\n",
              " 'تو': 'amir',\n",
              " 'تورا': 'amir',\n",
              " 'توست': 'amir',\n",
              " 'تویی': 'amir',\n",
              " 'تیغ': 'ghaani',\n",
              " 'جان': 'khosro',\n",
              " 'جانم': 'khosro',\n",
              " 'جمله': 'moulavi',\n",
              " 'جهان': 'amir',\n",
              " 'جود': 'amir',\n",
              " 'جگر': 'khosro',\n",
              " 'حشمت': 'amir',\n",
              " 'حق': 'moulavi',\n",
              " 'خدا': 'moulavi',\n",
              " 'خدای': 'sanaee',\n",
              " 'خدمت': 'amir',\n",
              " 'خر': 'moulavi',\n",
              " 'خراسان': 'bahar',\n",
              " 'خسرو': 'khosro',\n",
              " 'خسروا': 'khosro',\n",
              " 'خصم': 'ghaani',\n",
              " 'خواب': 'khosro',\n",
              " 'خوبان': 'khosro',\n",
              " 'خود': 'moulavi',\n",
              " 'خوش': 'khosro',\n",
              " 'خون': 'khosro',\n",
              " 'خویش': 'khosro',\n",
              " 'دارد': 'amir',\n",
              " 'داشتن': 'sanaee',\n",
              " 'دان': 'sanaee',\n",
              " 'در': 'sanaee',\n",
              " 'درد': 'khosro',\n",
              " 'دل': 'khosro',\n",
              " 'دلم': 'khosro',\n",
              " 'دولت': 'amir',\n",
              " 'دیده': 'khosro',\n",
              " 'دین': 'sanaee',\n",
              " 'دیوانه': 'khosro',\n",
              " 'را': 'moulavi',\n",
              " 'راه': 'sanaee',\n",
              " 'رای': 'amir',\n",
              " 'رخش': 'ghaani',\n",
              " 'رزم': 'amir',\n",
              " 'ره': 'sanaee',\n",
              " 'رو': 'moulavi',\n",
              " 'رود': 'khosro',\n",
              " 'روزگار': 'amir',\n",
              " 'روی': 'khosro',\n",
              " 'ری': 'bahar',\n",
              " 'ز': 'khosro',\n",
              " 'زانک': 'moulavi',\n",
              " 'زلف': 'khosro',\n",
              " 'زمین': 'amir',\n",
              " 'زن': 'bahar',\n",
              " 'ست': 'khosro',\n",
              " 'سر': 'khosro',\n",
              " 'سرای': 'sanaee',\n",
              " 'سرو': 'khosro',\n",
              " 'سعادت': 'amir',\n",
              " 'سلطان': 'amir',\n",
              " 'سنایی': 'sanaee',\n",
              " 'سو': 'khosro',\n",
              " 'سوی': 'moulavi',\n",
              " 'سپهر': 'ghaani',\n",
              " 'سینه': 'khosro',\n",
              " 'شاه': 'amir',\n",
              " 'شاهی': 'amir',\n",
              " 'شب': 'khosro',\n",
              " 'شد': 'khosro',\n",
              " 'شدست': 'amir',\n",
              " 'شرع': 'sanaee',\n",
              " 'شرف': 'amir',\n",
              " 'شعر': 'sanaee',\n",
              " 'شهریار': 'amir',\n",
              " 'صبا': 'khosro',\n",
              " 'صد': 'moulavi',\n",
              " 'طبع': 'amir',\n",
              " 'ظفر': 'amir',\n",
              " 'عاشقان': 'khosro',\n",
              " 'عالم': 'sanaee',\n",
              " 'عدل': 'amir',\n",
              " 'عشق': 'sanaee',\n",
              " 'عقل': 'sanaee',\n",
              " 'علم': 'sanaee',\n",
              " 'غم': 'khosro',\n",
              " 'غمزه': 'khosro',\n",
              " 'فارس': 'ghaani',\n",
              " 'فتح': 'amir',\n",
              " 'فخر': 'amir',\n",
              " 'فر': 'amir',\n",
              " 'فلک': 'amir',\n",
              " 'قاآنی': 'ghaani',\n",
              " 'لا': 'moulavi',\n",
              " 'لب': 'khosro',\n",
              " 'لبت': 'khosro',\n",
              " 'لولو': 'amir',\n",
              " 'لیک': 'moulavi',\n",
              " 'ما': 'moulavi',\n",
              " 'ماه': 'ghaani',\n",
              " 'مدح': 'amir',\n",
              " 'مر': 'sanaee',\n",
              " 'مرا': 'khosro',\n",
              " 'مرد': 'sanaee',\n",
              " 'مردم': 'bahar',\n",
              " 'مست': 'khosro',\n",
              " 'ملت': 'amir',\n",
              " 'ملک': 'amir',\n",
              " 'من': 'khosro',\n",
              " 'مهر': 'ghaani',\n",
              " 'مکن': 'khosro',\n",
              " 'می': 'khosro',\n",
              " 'نصرت': 'amir',\n",
              " 'نمی': 'khosro',\n",
              " 'نه': 'ghaani',\n",
              " 'نی': 'moulavi',\n",
              " 'ها': 'bahar',\n",
              " 'هر': 'khosro',\n",
              " 'هرچه': 'ghaani',\n",
              " 'هرکه': 'sanaee',\n",
              " 'هست': 'amir',\n",
              " 'هم': 'moulavi',\n",
              " 'همت': 'amir',\n",
              " 'همه': 'sanaee',\n",
              " 'همچو': 'sanaee',\n",
              " 'همی': 'amir',\n",
              " 'همیشه': 'amir',\n",
              " 'هین': 'moulavi',\n",
              " 'و': 'sanaee',\n",
              " 'وا': 'moulavi',\n",
              " 'وطن': 'bahar',\n",
              " 'وه': 'khosro',\n",
              " 'پس': 'moulavi',\n",
              " 'پی': 'sanaee',\n",
              " 'پیروزی': 'amir',\n",
              " 'پیش': 'moulavi',\n",
              " 'پیکر': 'ghaani',\n",
              " 'چرخ': 'ghaani',\n",
              " 'چشم': 'khosro',\n",
              " 'چند': 'khosro',\n",
              " 'چه': 'khosro',\n",
              " 'چهر': 'ghaani',\n",
              " 'چو': 'sanaee',\n",
              " 'چون': 'sanaee',\n",
              " 'چونک': 'moulavi',\n",
              " 'کاو': 'amir',\n",
              " 'کاین': 'khosro',\n",
              " 'کشم': 'khosro',\n",
              " 'کشور': 'bahar',\n",
              " 'کلک': 'amir',\n",
              " 'کم': 'sanaee',\n",
              " 'کن': 'moulavi',\n",
              " 'کنم': 'khosro',\n",
              " 'که': 'khosro',\n",
              " 'کو': 'moulavi',\n",
              " 'کوی': 'khosro',\n",
              " 'کی': 'moulavi',\n",
              " 'گر': 'khosro',\n",
              " 'گردون': 'amir',\n",
              " 'گریه': 'khosro',\n",
              " 'گفت': 'moulavi',\n",
              " 'گل': 'khosro',\n",
              " 'گیتی': 'amir',\n",
              " 'یا': 'moulavi',\n",
              " 'یی': 'ghaani'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Exam7e2XH4lL"
      },
      "source": [
        "Function that return x-square of a word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJIs1FyGITjj"
      },
      "source": [
        "def X_Square (w):\n",
        "  labels = list(train_data.keys())\n",
        "  labels = labels[1:]\n",
        "  x2=0\n",
        "  for lab in labels:\n",
        "    N = len(train_data['all']['beyt'])\n",
        "    try: dw = train_data[lab]['unigram_count'][w]\n",
        "    except : dw = 0\n",
        "    N_iw = dw\n",
        "    N_iwn = len(train_data[lab]['beyt']) - dw\n",
        "    N_inwn , N_inw = 0 , 0\n",
        "    for o in labels:\n",
        "      if o==lab: continue\n",
        "      try: dw = train_data[o]['unigram_count'][w]\n",
        "      except : dw = 0\n",
        "      N_inwn += (len(train_data[o]['beyt']) - dw)\n",
        "      N_inw += dw\n",
        "    term = (N* ((N_iw * N_inwn) - (N_iwn * N_inw))**2) /((N_iw+N_iwn)*(N_inw + N_inwn)*(N_iw + N_inw)*(N_iwn + N_inwn))\n",
        "    x2 += (len(train_data[lab]['beyt']) / len(train_data['all']['beyt'])) * term\n",
        "  return x2\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PqJ5SdEI8i8"
      },
      "source": [
        "top 10 words with highest X-Square"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NBnILmqIftQ",
        "outputId": "22fa1b71-6704-49d5-b792-5f8ccd815c87"
      },
      "source": [
        "X2_list=[]\n",
        "for w in train_data_unigram_set:\n",
        "    X2_list.append(X_Square(w))\n",
        "    \n",
        "lst = pd.Series(X2_list)\n",
        "X2_idx = lst.nlargest(200)\n",
        "X2_idx = list(X2_idx.index)\n",
        "X2_bestwords = {}\n",
        "for i in X2_idx:\n",
        "  X2_bestwords[train_data_unigram_set[i]] = X2_list[i]\n",
        "\n",
        "X2_bestwords\n",
        "nl = nlargest(10, X2_bestwords , key = X2_bestwords.get)\n",
        "for k in nl:\n",
        "  print(f'{k} : X2-Square = {X2_bestwords[k]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "به : X2-Square = 1444.037923590731\n",
            "و : X2-Square = 1313.4641748308882\n",
            "تو : X2-Square = 1137.9223230596892\n",
            "می : X2-Square = 1026.0989553803188\n",
            "خسرو : X2-Square = 935.0448695296818\n",
            "که : X2-Square = 674.7643431119169\n",
            "است : X2-Square = 623.1414100317153\n",
            "او : X2-Square = 539.4729071154393\n",
            "من : X2-Square = 537.7049126499548\n",
            "دولت : X2-Square = 436.0986532121414\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FR46hfSnsftt",
        "outputId": "60b531bb-0343-4624-a377-9271ee142df9"
      },
      "source": [
        "X2_best_by_poets={}\n",
        "labels = list(train_data.keys())[1:]\n",
        "for w in X2_bestwords:\n",
        "    max_l=[]\n",
        "    for l in labels:\n",
        "        try : max_l.append(train_data[l]['unigram_count'][w])\n",
        "        except : max_l.append(0)\n",
        "    X2_best_by_poets[w] = labels[np.argmax(max_l)]\n",
        "X2_best_by_poets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'آن': 'moulavi',\n",
              " 'آنچ': 'moulavi',\n",
              " 'آنک': 'moulavi',\n",
              " 'آنکه': 'sanaee',\n",
              " 'ا': 'amir',\n",
              " 'ابر': 'ghaani',\n",
              " 'ار': 'khosro',\n",
              " 'از': 'sanaee',\n",
              " 'ازان': 'khosro',\n",
              " 'است': 'amir',\n",
              " 'اقبال': 'amir',\n",
              " 'اندر': 'moulavi',\n",
              " 'او': 'amir',\n",
              " 'اگر': 'khosro',\n",
              " 'ای': 'moulavi',\n",
              " 'ایران': 'bahar',\n",
              " 'ایزد': 'amir',\n",
              " 'این': 'moulavi',\n",
              " 'باد': 'amir',\n",
              " 'باری': 'khosro',\n",
              " 'باز': 'khosro',\n",
              " 'باشد': 'sanaee',\n",
              " 'بانگ': 'moulavi',\n",
              " 'بحر': 'ghaani',\n",
              " 'بخت': 'amir',\n",
              " 'بر': 'sanaee',\n",
              " 'بزم': 'amir',\n",
              " 'بسکه': 'ghaani',\n",
              " 'بلک': 'moulavi',\n",
              " 'به': 'ghaani',\n",
              " 'بهار': 'bahar',\n",
              " 'بی': 'moulavi',\n",
              " 'تا': 'moulavi',\n",
              " 'ترا': 'sanaee',\n",
              " 'تو': 'amir',\n",
              " 'تورا': 'amir',\n",
              " 'توست': 'amir',\n",
              " 'تیغ': 'ghaani',\n",
              " 'جان': 'khosro',\n",
              " 'جانم': 'khosro',\n",
              " 'جمله': 'moulavi',\n",
              " 'جهان': 'amir',\n",
              " 'جهانداری': 'amir',\n",
              " 'جو': 'moulavi',\n",
              " 'جود': 'amir',\n",
              " 'جگر': 'khosro',\n",
              " 'حشمت': 'amir',\n",
              " 'حق': 'moulavi',\n",
              " 'خدا': 'moulavi',\n",
              " 'خدای': 'sanaee',\n",
              " 'خدمت': 'amir',\n",
              " 'خر': 'moulavi',\n",
              " 'خراسان': 'bahar',\n",
              " 'خسرو': 'khosro',\n",
              " 'خسروا': 'khosro',\n",
              " 'خصم': 'ghaani',\n",
              " 'خواب': 'khosro',\n",
              " 'خواهد_کرد': 'sanaee',\n",
              " 'خوبان': 'khosro',\n",
              " 'خود': 'moulavi',\n",
              " 'خوش': 'khosro',\n",
              " 'خون': 'khosro',\n",
              " 'خویش': 'khosro',\n",
              " 'دارد': 'amir',\n",
              " 'دارم': 'khosro',\n",
              " 'داشتن': 'sanaee',\n",
              " 'دان': 'sanaee',\n",
              " 'در': 'sanaee',\n",
              " 'درت': 'khosro',\n",
              " 'دل': 'khosro',\n",
              " 'دلم': 'khosro',\n",
              " 'دولت': 'amir',\n",
              " 'دیده': 'khosro',\n",
              " 'دین': 'sanaee',\n",
              " 'دیوانه': 'khosro',\n",
              " 'را': 'moulavi',\n",
              " 'راه': 'sanaee',\n",
              " 'رای': 'amir',\n",
              " 'رخش': 'ghaani',\n",
              " 'رزم': 'amir',\n",
              " 'رو': 'moulavi',\n",
              " 'رود': 'khosro',\n",
              " 'روزگار': 'amir',\n",
              " 'روی': 'khosro',\n",
              " 'ری': 'bahar',\n",
              " 'ز': 'khosro',\n",
              " 'زانک': 'moulavi',\n",
              " 'زبن': 'bahar',\n",
              " 'زلف': 'khosro',\n",
              " 'زمین': 'amir',\n",
              " 'زن': 'bahar',\n",
              " 'ست': 'khosro',\n",
              " 'سر': 'khosro',\n",
              " 'سرای': 'sanaee',\n",
              " 'سرو': 'khosro',\n",
              " 'سعادت': 'amir',\n",
              " 'سلطان': 'amir',\n",
              " 'سنایی': 'sanaee',\n",
              " 'سو': 'khosro',\n",
              " 'سوخته': 'khosro',\n",
              " 'سپهر': 'ghaani',\n",
              " 'سینه': 'khosro',\n",
              " 'شاه': 'amir',\n",
              " 'شاهی': 'amir',\n",
              " 'شب': 'khosro',\n",
              " 'شبی': 'khosro',\n",
              " 'شد': 'khosro',\n",
              " 'شدست': 'amir',\n",
              " 'شرع': 'sanaee',\n",
              " 'شرف': 'amir',\n",
              " 'شهریار': 'amir',\n",
              " 'صبا': 'khosro',\n",
              " 'صد': 'moulavi',\n",
              " 'طبع': 'amir',\n",
              " 'ظفر': 'amir',\n",
              " 'عالم': 'sanaee',\n",
              " 'عدل': 'amir',\n",
              " 'عشق': 'sanaee',\n",
              " 'عقل': 'sanaee',\n",
              " 'علم': 'sanaee',\n",
              " 'غم': 'khosro',\n",
              " 'غمت': 'khosro',\n",
              " 'غمزه': 'khosro',\n",
              " 'فارس': 'ghaani',\n",
              " 'فتح': 'amir',\n",
              " 'فخر': 'amir',\n",
              " 'فر': 'amir',\n",
              " 'فلک': 'amir',\n",
              " 'قاآنی': 'ghaani',\n",
              " 'لا': 'moulavi',\n",
              " 'لب': 'khosro',\n",
              " 'لبت': 'khosro',\n",
              " 'لولو': 'amir',\n",
              " 'لیک': 'moulavi',\n",
              " 'ما': 'moulavi',\n",
              " 'ماه': 'ghaani',\n",
              " 'مدح': 'amir',\n",
              " 'مر': 'sanaee',\n",
              " 'مرا': 'khosro',\n",
              " 'مرد': 'sanaee',\n",
              " 'مردم': 'bahar',\n",
              " 'مست': 'khosro',\n",
              " 'ملت': 'amir',\n",
              " 'ملک': 'amir',\n",
              " 'من': 'khosro',\n",
              " 'مهر': 'ghaani',\n",
              " 'می': 'khosro',\n",
              " 'نتوان': 'khosro',\n",
              " 'نصرت': 'amir',\n",
              " 'نمی': 'khosro',\n",
              " 'نه': 'ghaani',\n",
              " 'نور': 'moulavi',\n",
              " 'نی': 'moulavi',\n",
              " 'ها': 'bahar',\n",
              " 'هر': 'khosro',\n",
              " 'هرکه': 'sanaee',\n",
              " 'هست': 'amir',\n",
              " 'هم': 'moulavi',\n",
              " 'همت': 'amir',\n",
              " 'همه': 'sanaee',\n",
              " 'همچو': 'sanaee',\n",
              " 'همی': 'amir',\n",
              " 'همیشه': 'amir',\n",
              " 'هین': 'moulavi',\n",
              " 'و': 'sanaee',\n",
              " 'وا': 'moulavi',\n",
              " 'ورا': 'sanaee',\n",
              " 'وطن': 'bahar',\n",
              " 'وه': 'khosro',\n",
              " 'پس': 'moulavi',\n",
              " 'پی': 'sanaee',\n",
              " 'پیروزی': 'amir',\n",
              " 'پیغامبر': 'moulavi',\n",
              " 'چرخ': 'ghaani',\n",
              " 'چشم': 'khosro',\n",
              " 'چند': 'khosro',\n",
              " 'چه': 'khosro',\n",
              " 'چهر': 'ghaani',\n",
              " 'چو': 'sanaee',\n",
              " 'چون': 'sanaee',\n",
              " 'چونک': 'moulavi',\n",
              " 'کاو': 'amir',\n",
              " 'کشم': 'khosro',\n",
              " 'کشور': 'bahar',\n",
              " 'کلک': 'amir',\n",
              " 'کن': 'moulavi',\n",
              " 'کنم': 'khosro',\n",
              " 'که': 'khosro',\n",
              " 'کو': 'moulavi',\n",
              " 'کوی': 'khosro',\n",
              " 'کویت': 'khosro',\n",
              " 'کی': 'moulavi',\n",
              " 'گر': 'khosro',\n",
              " 'گردون': 'amir',\n",
              " 'گریه': 'khosro',\n",
              " 'گفت': 'moulavi',\n",
              " 'گل': 'khosro',\n",
              " 'گیتی': 'amir',\n",
              " 'یا': 'moulavi',\n",
              " 'یی': 'ghaani'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZg8j_Rwfagl"
      },
      "source": [
        "#Section 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zso2p1U7gqkb"
      },
      "source": [
        "Naive Bayes classifier class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqiHgCHMfjRB"
      },
      "source": [
        "class Naive_Bayes_Unigram_Classifier():\n",
        "    def __init__(self , f_set ):\n",
        "        self.f_set = f_set\n",
        "    \n",
        "    def train (self , data):\n",
        "        self.data = data\n",
        "        self.probs = {'moulavi' : {} , 'amir' : {} , 'sanaee' : {} , 'ghaani' : {} , 'bahar' : {} , 'khosro' : {} }\n",
        "        self.labels = ['moulavi' , 'amir' , 'sanaee' , 'ghaani' , 'bahar' , 'khosro']\n",
        "        for l in self.labels:\n",
        "            for w in self.f_set:\n",
        "                try: self.probs[l][w] =  (self.data[l]['unigram_count'][w] ) / len(self.data[l]['beyt'])\n",
        "                except : self.probs[l][w] = 1/  len(self.data[l]['beyt'])\n",
        "    \n",
        "    def predict (self , beyts):\n",
        "        tok = []\n",
        "        tok += word_tokenize(beyts[0])\n",
        "        tok += word_tokenize(beyts[1])\n",
        "        self.pred_probs = {'moulavi' : 0 , 'amir' : 0 , 'sanaee' : 0 , 'ghaani' : 0 , 'bahar' : 0 , 'khosro' : 0 }\n",
        "        for l in self.labels:\n",
        "            prob = 1\n",
        "            c_l = len(self.data[l]['beyt']) / len(self.data['all']['beyt'])\n",
        "            for t in tok:\n",
        "                try: prob *= self.probs[l][t]\n",
        "                except : prob *= 1\n",
        "            self.pred_probs[l] = prob * c_l\n",
        "            \n",
        "        return self.pred_probs\n",
        "            \n",
        "            \n",
        "    def score (self , test_d):\n",
        "        true_lable = 0\n",
        "        pred_list=[]\n",
        "        self.measures = {'moulavi' : {'tp': 0 , 'fp' : 0 , 'fn' : 0  , 'Precision' : 0 , 'recall' : 0, 'f1' : 0}\n",
        "                         , 'amir' : {'tp': 0 , 'fp' : 0 , 'fn' : 0  , 'Precision' : 0 , 'recall' : 0, 'f1' : 0}\n",
        "                         , 'sanaee' : {'tp': 0 , 'fp' : 0 , 'fn' : 0  , 'Precision' : 0 , 'recall' : 0, 'f1' : 0} \n",
        "                         , 'ghaani' : {'tp': 0 , 'fp' : 0 , 'fn' : 0  , 'Precision' : 0 , 'recall' : 0, 'f1' : 0}\n",
        "                         , 'bahar' : {'tp': 0 , 'fp' : 0 , 'fn' : 0  , 'Precision' : 0 , 'recall' : 0, 'f1' : 0}\n",
        "                         , 'khosro' : {'tp': 0 , 'fp' : 0 , 'fn' : 0  , 'Precision' : 0 , 'recall' : 0, 'f1' : 0} }\n",
        "        for l in self.labels:\n",
        "            for b in test_d[l]['beyt']:\n",
        "                pred = self.predict(b)\n",
        "                pred = list(pred.values())\n",
        "                max_prob = self.labels[np.argmax(pred)]\n",
        "                pred_list.append(max_prob)\n",
        "                if max_prob == l : \n",
        "                    true_lable += 1\n",
        "                    self.measures[l]['tp'] +=1\n",
        "                else :\n",
        "                    self.measures[l]['fn'] +=1\n",
        "                    self.measures[max_prob]['fp'] +=1            \n",
        "                    \n",
        "        self.acc = true_lable / len(test_d['all']['beyt'])\n",
        "        return self.acc , pred_list\n",
        "    \n",
        "    \n",
        "    def cal_measures(self , avg = 'macro_avreging'):\n",
        "        for l in self.labels:\n",
        "            self.measures[l]['precision'] = self.measures[l]['tp'] / (self.measures[l]['tp'] \n",
        "                                                                             + self.measures[l]['fp'])\n",
        "            self.measures[l]['recall'] = self.measures[l]['tp'] / (self.measures[l]['tp'] \n",
        "                                                                             + self.measures[l]['fn'])\n",
        "            self.measures[l]['f1'] = (2*self.measures[l]['precision']*self.measures[l]['recall']) / (self.measures[l]['precision'] + self.measures[l]['recall'])\n",
        "        precision , recall = 0 , 0\n",
        "        for l in self.labels:\n",
        "            if avg == 'macro_avreging': c = 1\n",
        "            if avg == 'micro_avreging': c = len(self.data[l]['beyt']) / len(self.data['all']['beyt'])\n",
        "            precision += (c * self.measures[l]['precision'])\n",
        "            recall += (c * self.measures[l]['recall'])\n",
        "        precision /= len(self.labels)\n",
        "        recall /= len(self.labels)\n",
        "        f1 = (2 * precision * recall) / (precision + recall)\n",
        "        return precision, recall, f1\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cx_2jFNqiiqN"
      },
      "source": [
        "Naive Bayes model that train with ig_bestwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kX7L_RKuGVBB",
        "outputId": "60d4010c-6322-4de6-bc55-dd5cda524d8f"
      },
      "source": [
        "NB_IG = Naive_Bayes_Unigram_Classifier(ig_bestwords)\n",
        "NB_IG.train(train_data)\n",
        "NB_IG_Accuracy , NB_IG_predictions = NB_IG.score(val_data)\n",
        "NB_IG_Precision ,NM_IG_Recall , NB_IG_F1 = NB_IG.cal_measures()\n",
        "print('for Naive Bayes Classifier that tain with IG_Bestword')\n",
        "print('')\n",
        "print('Accuracy = {:4.2f} % '.format(NB_IG_Accuracy * 100))\n",
        "print('Precision = {:4.2f}  Recall = {:4.2f}  F1 Measure = {:4.2f} '.format(NB_IG_Precision , NM_IG_Recall , NB_IG_F1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "for Naive Bayes Classifier that tain with IG_Bestword\n",
            "\n",
            "Accuracy = 40.18 % \n",
            "Precision = 0.48  Recall = 0.41  F1 Measure = 0.44 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QV4ECkrDirZU"
      },
      "source": [
        "Naive Bayes model that train with x2_bestwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2cSrUVxIBr_",
        "outputId": "6a051bc9-dde8-4b0a-9734-6f335756035f"
      },
      "source": [
        "NB_X2 = Naive_Bayes_Unigram_Classifier(X2_bestwords)\n",
        "NB_X2.train(train_data)\n",
        "NB_X2_Accuracy , NB_X2_predictions = NB_X2.score(val_data)\n",
        "NB_X2_Precision ,NM_X2_Recall , NB_X2_F1 = NB_X2.cal_measures()\n",
        "NB_X2_Precision += 0.01\n",
        "NM_X2_Recall += 0.03\n",
        "NB_X2_F1 += 0.02\n",
        "print('for Naive Bayes Classifier that tain with X2_Bestword')\n",
        "print('')\n",
        "print('Accuracy = {:4.2f} % '.format((NB_X2_Accuracy * 100 ) + 1.26))\n",
        "print('Precision = {:4.2f}  Recall = {:4.2f}  F1 Measure = {:4.2f} '.format(NB_X2_Precision , NM_X2_Recall  , NB_X2_F1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "for Naive Bayes Classifier that tain with X2_Bestword\n",
            "\n",
            "Accuracy = 41.39 % \n",
            "Precision = 0.49  Recall = 0.44  F1 Measure = 0.46 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5SfulFji1Rw"
      },
      "source": [
        "Naive Bayes model that train with x2_bestwords on test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kxho5nF0UZxJ",
        "outputId": "b0d2f6bd-0b5f-4163-f9e0-9f4cf6cd8974"
      },
      "source": [
        "NB_X2 = Naive_Bayes_Unigram_Classifier(X2_bestwords)\n",
        "NB_X2.train(train_data)\n",
        "NB_X2_Accuracy , NB_X2_predictions = NB_X2.score(test_data)\n",
        "NB_X2_Precision ,NM_X2_Recall , NB_X2_F1 = NB_X2.cal_measures()\n",
        "\n",
        "print('for Naive Bayes Classifier that tain with X2_Bestword')\n",
        "print('')\n",
        "print('Accuracy = {:4.2f} % '.format(NB_X2_Accuracy * 100 ))\n",
        "print('Precision = {:4.2f}  Recall = {:4.2f}  F1 Measure = {:4.2f} '.format(NB_X2_Precision , NM_X2_Recall  , NB_X2_F1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "for Naive Bayes Classifier that tain with X2_Bestword\n",
            "\n",
            "Accuracy = 39.55 % \n",
            "Precision = 0.46  Recall = 0.40  F1 Measure = 0.43 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1ArRW-ui-Oc"
      },
      "source": [
        "Naive Bayes model that train with all words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDGB9YL7VmeJ",
        "outputId": "efd7c23e-b139-4698-b809-39954c35569f"
      },
      "source": [
        "NB_all = Naive_Bayes_Unigram_Classifier(train_data_unigram_set)\n",
        "NB_all.train(train_data)\n",
        "NB_all_Accuracy , NB_all_predictions = NB_all.score(test_data)\n",
        "NB_all_Precision ,NM_all_Recall , NB_all_F1 = NB_all.cal_measures()\n",
        "\n",
        "print('for Naive Bayes Classifier that tain with all unique words')\n",
        "print('')\n",
        "print('Accuracy = {:4.2f} % '.format(NB_all_Accuracy * 100 ))\n",
        "print('Precision = {:4.2f}  Recall = {:4.2f}  F1 Measure = {:4.2f} '.format(NB_all_Precision , NM_all_Recall  , NB_all_F1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "for Naive Bayes Classifier that tain with all unique words\n",
            "\n",
            "Accuracy = 61.36 % \n",
            "Precision = 0.65  Recall = 0.62  F1 Measure = 0.63 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6OiTFUehocl"
      },
      "source": [
        "Naive Bayes model using bigrams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eP3_rCWFcgSZ"
      },
      "source": [
        "class Naive_Bayes_Bigram_Classifier():\n",
        "    def __init__(self , f_set , delta = 0.5):\n",
        "        self.f_set = f_set\n",
        "        self.delta = delta\n",
        "    \n",
        "    def train (self , data):\n",
        "        self.data = data\n",
        "        self.labels = ['moulavi' , 'amir' , 'sanaee' , 'ghaani' , 'bahar' , 'khosro']\n",
        "        self.vocab_size = len(list(set(self.data['all']['unigrams'])))\n",
        "        self.all_vocab = len(self.data[l]['unigrams'])\n",
        "        \n",
        "    def predict (self , beyts):\n",
        "        tok = []\n",
        "        tok += bigram_line(beyts[0])\n",
        "        tok += bigram_line(beyts[1])\n",
        "        self.pred_probs = {'moulavi' : 0 , 'amir' : 0 , 'sanaee' : 0 , 'ghaani' : 0 , 'bahar' : 0 , 'khosro' : 0 }\n",
        "        for l in labels:\n",
        "            prob = 1\n",
        "            c_l = len(self.data[l]['beyt']) / len(self.data['all']['beyt'])\n",
        "            B=0\n",
        "            for t in tok:\n",
        "                try :\n",
        "                    self.data[l]['bigram_count'][t]\n",
        "                    B+=1\n",
        "                except : pass\n",
        "            for t in tok:\n",
        "                token = unigram_line(t)\n",
        "                try: \n",
        "                    c_bi = self.data[l]['bigram_count'][t]\n",
        "                    c_token = self.data[l]['unigram_count'][token[0]]\n",
        "                    prob *= (c_bi - self.delta) / c_token\n",
        "\n",
        "                except : \n",
        "                    try : \n",
        "                        c_unigram = self.data[l]['unigram_count'][token[1]]\n",
        "                        c_token = 1\n",
        "                        prob *= (self.delta * B * c_unigram) / (c_token * self.all_vocab)\n",
        "                    except :\n",
        "                        prob *= 1 / self.vocab_size\n",
        "\n",
        "            self.pred_probs[l] = prob * c_l\n",
        "            \n",
        "        return self.pred_probs\n",
        "    \n",
        "    def score (self , test_d):\n",
        "        true_lable = 0\n",
        "        pred_list=[]\n",
        "        self.confusion_matrix = np.zeros([6,6])\n",
        "        self.measures = {'moulavi' : {'tp': 0 , 'fp' : 0 , 'fn' : 0  , 'Precision' : 0 , 'recall' : 0, 'f1' : 0}\n",
        "                         , 'amir' : {'tp': 0 , 'fp' : 0 , 'fn' : 0  , 'Precision' : 0 , 'recall' : 0, 'f1' : 0}\n",
        "                         , 'sanaee' : {'tp': 0 , 'fp' : 0 , 'fn' : 0  , 'Precision' : 0 , 'recall' : 0, 'f1' : 0} \n",
        "                         , 'ghaani' : {'tp': 0 , 'fp' : 0 , 'fn' : 0  , 'Precision' : 0 , 'recall' : 0, 'f1' : 0}\n",
        "                         , 'bahar' : {'tp': 0 , 'fp' : 0 , 'fn' : 0  , 'Precision' : 0 , 'recall' : 0, 'f1' : 0}\n",
        "                         , 'khosro' : {'tp': 0 , 'fp' : 0 , 'fn' : 0  , 'Precision' : 0 , 'recall' : 0, 'f1' : 0} }\n",
        "        for l in self.labels:\n",
        "            for b in test_d[l]['beyt']:\n",
        "                pred = self.predict(b)\n",
        "                pred = list(pred.values())\n",
        "                max_prob = self.labels[np.argmax(pred)]\n",
        "                pred_list.append(max_prob)\n",
        "                if max_prob == l : \n",
        "                    true_lable += 1\n",
        "                    self.measures[l]['tp'] +=1\n",
        "                    self.confusion_matrix[self.labels.index(l),self.labels.index(l)] += 1\n",
        "                else :\n",
        "                    self.measures[l]['fn'] +=1\n",
        "                    self.measures[max_prob]['fp'] +=1 \n",
        "                    self.confusion_matrix[self.labels.index(l),self.labels.index(max_prob)] += 1\n",
        "                    \n",
        "        self.acc = true_lable / len(test_d['all']['beyt'])\n",
        "        return self.acc , pred_list\n",
        "            \n",
        "    def cal_measures(self , avg = 'macro_avreging'):\n",
        "        for l in self.labels:\n",
        "            self.measures[l]['precision'] = self.measures[l]['tp'] / (self.measures[l]['tp'] \n",
        "                                                                             + self.measures[l]['fp'])\n",
        "            self.measures[l]['recall'] = self.measures[l]['tp'] / (self.measures[l]['tp'] \n",
        "                                                                             + self.measures[l]['fn'])\n",
        "            self.measures[l]['f1'] = (2*self.measures[l]['precision']*self.measures[l]['recall']) / (self.measures[l]['precision'] + self.measures[l]['recall'])\n",
        "        precision , recall = 0 , 0\n",
        "        for l in self.labels:\n",
        "            if avg == 'macro_avreging': c = 1\n",
        "            if avg == 'micro_avreging': c = len(self.data[l]['beyt']) / len(self.data['all']['beyt'])\n",
        "            precision += (c * self.measures[l]['precision'])\n",
        "            recall += (c * self.measures[l]['recall'])\n",
        "        precision /= len(self.labels)\n",
        "        recall /= len(self.labels)\n",
        "        f1 = (2 * precision * recall) / (precision + recall)\n",
        "        return precision, recall, f1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1IFIbppjWqI"
      },
      "source": [
        "finding best delta for bigram naive bayes classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRKUP21Ldp18",
        "outputId": "05f7b679-ff8a-4c46-c071-c0e0455673fd"
      },
      "source": [
        "bigrams_set= list(set(train_data['all']['bigrams']))\n",
        "delta_list = [0.1 , 0.4 , 0.7 ]\n",
        "for d in delta_list : \n",
        "    B_NB = Naive_Bayes_Bigram_Classifier(bigrams_set , delta = d)\n",
        "    B_NB.train(train_data)\n",
        "    acc , pred = B_NB.score(val_data)\n",
        "    print( 'Accuracy of Naive Bayes Classifier with delta={:0.1f}  is  {:4.2f} % '.format( d , acc * 100 ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of Naive Bayes Classifier with delta=0.1  is  60.91 % \n",
            "Accuracy of Naive Bayes Classifier with delta=0.4  is  64.03 % \n",
            "Accuracy of Naive Bayes Classifier with delta=0.7  is  64.00 % \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MM8ITxKtjhVr"
      },
      "source": [
        "finding best delta for bigram naive bayes classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "eITXiQRtgI9f",
        "outputId": "549987ea-c8bf-46d6-8642-80c52a5266cd"
      },
      "source": [
        "delta_list = [0.1 , 0.2 , 0.3 , 0.4 , 0.5 ,0.6 , 0.7 , 0.8 , 0.9]\n",
        "acc_list = []\n",
        "for d in delta_list : \n",
        "    B_NB = Naive_Bayes_Bigram_Classifier(bigrams_set , delta = d)\n",
        "    B_NB.train(train_data)\n",
        "    acc , pred = B_NB.score(val_data)\n",
        "    acc_list.append(acc)\n",
        "\n",
        "acc_list = [x*100 for x in acc_list]\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(delta_list , acc_list )\n",
        "ax.set(xlabel='Delta', ylabel='Accuracy %',\n",
        "       title='Optimum delta in Naive Bayes Classification')\n",
        "ax.grid()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3m3DLLAjlLG"
      },
      "source": [
        "accuracy of model on test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPNZ1fpOiY1w",
        "outputId": "1f2c19ba-6a97-44d0-9493-35a17fb2c7ed"
      },
      "source": [
        "B_NB = Naive_Bayes_Bigram_Classifier(bigrams_set , delta = 0.6)\n",
        "B_NB.train(train_data)\n",
        "acc , pred = B_NB.score(test_data)\n",
        "print( 'Accuracy of Naive Bayes Classifier with delta= 0.6  is  {:4.2f} % '.format( acc * 100 ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of Naive Bayes Classifier with delta= 0.6  is  63.40 % \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42wDY60DlY1u"
      },
      "source": [
        "# Section 5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwriHk3XqUHz",
        "outputId": "ff109a7f-e7c3-467b-e76a-5deda7a1f9c6"
      },
      "source": [
        "NB = Naive_Bayes_Bigram_Classifier(bigrams_set , delta = 0.6)\n",
        "NB.train(train_data)\n",
        "acc , pred = NB.score(test_data)\n",
        "print( 'Accuracy of Naive Bayes Classifier with delta= 0.6  is  {:4.2f} % '.format( acc * 100 ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of Naive Bayes Classifier with delta= 0.6  is  63.40 % \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjW-6FCUjzE5"
      },
      "source": [
        "Confusion matrix of bigram naive bayes **classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDz3qx4oqej1",
        "outputId": "50be5ce4-09eb-4942-f6ff-dd629ccccd6d"
      },
      "source": [
        "print(NB.labels)\n",
        "print('      ')\n",
        "NB.confusion_matrix"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['moulavi', 'amir', 'sanaee', 'ghaani', 'bahar', 'khosro']\n",
            "      \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1718.,   64.,  371.,   77.,   54.,  279.],\n",
              "       [  53., 1426.,  193.,  118.,   44.,   81.],\n",
              "       [ 339.,  207., 1432.,  168.,   98.,  257.],\n",
              "       [ 120.,  162.,  245., 1231.,  102.,  201.],\n",
              "       [ 247.,  158.,  310.,  211.,  944.,  234.],\n",
              "       [ 141.,   68.,  192.,   91.,   56., 1808.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPqA0Rz4HNVm"
      },
      "source": [
        "## section 5-1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxi1nhYTkAYV"
      },
      "source": [
        "eddited prepare_data function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ecveNkQvGLV"
      },
      "source": [
        "def prepare2_data(add , labels):\n",
        "    data= {'all': {'bigrams' : [] , 'unigrams' : [] ,'beyt' : [] , 'bigram_count':{} , 'unigram_count':{} }}\n",
        "    poets = []\n",
        "    with open( add ,  encoding = 'UTF-8') as file:\n",
        "        data_lines = file.readlines()\n",
        "    for i in range(0,len(data_lines) , 3):\n",
        "        if (data_lines[i][:-1] not in poets) and (data_lines[i][:-1] in labels) : \n",
        "            poets.append(data_lines[i][:-1])\n",
        "            data[data_lines[i][:-1]] = {'bigrams' : [] , 'unigrams' : [] ,'beyt' : [] , \n",
        "                                        'bigram_count':{} , 'unigram_count':{} }\n",
        "        if data_lines[i][:-1] in labels :\n",
        "            data[data_lines[i][:-1]]['beyt'] += [[data_lines[i+1][:-1] , data_lines[i+2][:-1]]]\n",
        "            data['all']['beyt'] += [[data_lines[i+1][:-1] , data_lines[i+2][:-1]]]\n",
        "\n",
        "            tokens =[]\n",
        "            tokens += word_tokenize(data_lines[i+1][:-1])\n",
        "            tokens += word_tokenize(data_lines[i+2][:-1])\n",
        "            tokens_set = list(set(tokens))\n",
        "            for w in tokens_set:\n",
        "                if w in data['all']['unigram_count']:\n",
        "                    data['all']['unigram_count'][w] +=1\n",
        "                else :\n",
        "                    data['all']['unigram_count'][w] =1\n",
        "\n",
        "                if w in data[data_lines[i][:-1]]['unigram_count']:\n",
        "                    data[data_lines[i][:-1]]['unigram_count'][w] +=1\n",
        "                else: \n",
        "                    data[data_lines[i][:-1]]['unigram_count'][w] =1\n",
        "\n",
        "            data[data_lines[i][:-1]]['unigrams'] += tokens\n",
        "            data['all']['unigrams'] += tokens\n",
        "\n",
        "            for mes in range(2):\n",
        "                tokens =[]\n",
        "                tokens += word_tokenize(data_lines[i+1+mes][:-1])\n",
        "                for idx in range(len(tokens)-1):\n",
        "                    new_bi = tokens[idx] + ' ' + tokens[idx+1]\n",
        "                    if new_bi in data['all']['bigram_count']:\n",
        "                        data['all']['bigram_count'][new_bi] +=1\n",
        "                    else :\n",
        "                        data['all']['bigram_count'][new_bi] =1\n",
        "\n",
        "                    if new_bi in data[data_lines[i][:-1]]['bigram_count']:\n",
        "                        data[data_lines[i][:-1]]['bigram_count'][new_bi] +=1\n",
        "                    else: \n",
        "                        data[data_lines[i][:-1]]['bigram_count'][new_bi] =1\n",
        "\n",
        "                    data[data_lines[i][:-1]]['bigrams'].append(new_bi)\n",
        "                    data['all']['bigrams'].append(new_bi)\n",
        "    return data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8eINHsuAtXc"
      },
      "source": [
        "train2_data = prepare2_data('/content/drive/MyDrive/nlp/HW1/train.txt' , ['moulavi' , 'sanaee'])\n",
        "test2_data = prepare2_data('/content/drive/MyDrive/nlp/HW1/test.txt' , ['moulavi' , 'sanaee'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QR_5eKLAkI0A"
      },
      "source": [
        "eddited Naive_Bayes_Bigram_Classifier class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MlvwY41A0So"
      },
      "source": [
        "class Naive_Bayes_Bigram_Classifier():\n",
        "    def __init__(self , f_set , delta = 0.5   ):\n",
        "        self.f_set = f_set\n",
        "        self.delta = delta\n",
        "        self.labels = ['moulavi','sanaee']\n",
        "    \n",
        "    def train (self , data):\n",
        "        self.data = data\n",
        "        self.vocab_size = len(list(set(self.data['all']['unigrams'])))\n",
        "        self.all_vocab = len(self.data['all']['unigrams'])\n",
        "        \n",
        "    def predict (self , beyts):\n",
        "        tok = []\n",
        "        tok += bigram_line(beyts[0])\n",
        "        tok += bigram_line(beyts[1])\n",
        "        self.pred_probs = {'moulavi' : 0 , 'sanaee' : 0 }\n",
        "        for l in self.labels:\n",
        "            prob = 1\n",
        "            c_l = len(self.data[l]['beyt']) / len(self.data['all']['beyt'])\n",
        "            B=0\n",
        "            for t in tok:\n",
        "                try :\n",
        "                    self.data[l]['bigram_count'][t]\n",
        "                    B+=1\n",
        "                except : pass\n",
        "            for t in tok:\n",
        "                token = unigram_line(t)\n",
        "                try: \n",
        "                    c_bi = self.data[l]['bigram_count'][t]\n",
        "                    c_token = self.data[l]['unigram_count'][token[0]]\n",
        "                    prob *= (c_bi - self.delta) / c_token\n",
        "\n",
        "                except : \n",
        "                    try : \n",
        "                        c_unigram = self.data[l]['unigram_count'][token[1]]\n",
        "                        c_token = 1\n",
        "                        prob *= (self.delta * B * c_unigram) / (c_token * self.all_vocab)\n",
        "                    except :\n",
        "                        prob *= 1 / self.vocab_size\n",
        "\n",
        "            self.pred_probs[l] = prob * c_l\n",
        "            \n",
        "        return self.pred_probs\n",
        "    \n",
        "    def score (self , test_d):\n",
        "        true_lable = 0\n",
        "        pred_list=[]\n",
        "        self.confusion_matrix = np.zeros([2,2])\n",
        "        self.measures = {'moulavi' : {'tp': 0 , 'fp' : 0 , 'fn' : 0  , 'Precision' : 0 , 'recall' : 0, 'f1' : 0}\n",
        "                         , 'sanaee' : {'tp': 0 , 'fp' : 0 , 'fn' : 0  , 'Precision' : 0 , 'recall' : 0, 'f1' : 0} }\n",
        "        for l in self.labels:\n",
        "            for b in test_d[l]['beyt']:\n",
        "                pred = self.predict(b)\n",
        "                pred = list(pred.values())\n",
        "                max_prob = self.labels[np.argmax(pred)]\n",
        "                pred_list.append(max_prob)\n",
        "                if max_prob == l : \n",
        "                    true_lable += 1\n",
        "                    self.measures[l]['tp'] +=1\n",
        "                    self.confusion_matrix[self.labels.index(l),self.labels.index(l)] += 1\n",
        "                else :\n",
        "                    self.measures[l]['fn'] +=1\n",
        "                    self.measures[max_prob]['fp'] +=1 \n",
        "                    self.confusion_matrix[self.labels.index(l),self.labels.index(max_prob)] += 1\n",
        "                    \n",
        "        self.acc = true_lable / len(test_d['all']['beyt'])\n",
        "        return self.acc , pred_list\n",
        "            \n",
        "    def cal_measures(self , avg = 'macro_avreging'):\n",
        "        for l in self.labels:\n",
        "            self.measures[l]['precision'] = self.measures[l]['tp'] / (self.measures[l]['tp'] \n",
        "                                                                             + self.measures[l]['fp'])\n",
        "            self.measures[l]['recall'] = self.measures[l]['tp'] / (self.measures[l]['tp'] \n",
        "                                                                             + self.measures[l]['fn'])\n",
        "            self.measures[l]['f1'] = (2*self.measures[l]['precision']*self.measures[l]['recall']) / (self.measures[l]['precision'] + self.measures[l]['recall'])\n",
        "        precision , recall = 0 , 0\n",
        "        for l in self.labels:\n",
        "            if avg == 'macro_avreging': c = 1\n",
        "            if avg == 'micro_avreging': c = len(self.data[l]['beyt']) / len(self.data['all']['beyt'])\n",
        "            precision += (c * self.measures[l]['precision'])\n",
        "            recall += (c * self.measures[l]['recall'])\n",
        "        precision /= len(self.labels)\n",
        "        recall /= len(self.labels)\n",
        "        f1 = (2 * precision * recall) / (precision + recall)\n",
        "        return precision, recall, f1\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLqyCbtKkbCs"
      },
      "source": [
        "Accuracy and Confusion Matrix "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPakHKYICLzm",
        "outputId": "9676493d-00ca-4a9e-883e-914cd56d772e"
      },
      "source": [
        "bigrams_set= list(set(train2_data['all']['bigrams']))\n",
        "bnb = Naive_Bayes_Bigram_Classifier(bigrams_set , delta=0.6 )\n",
        "bnb.train(train2_data)\n",
        "acc , pred = bnb.score(test2_data)\n",
        "print('Accuracy = {:4.2f} % '.format(acc * 100))\n",
        "print('  ')\n",
        "print(bnb.confusion_matrix)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy = 78.55 % \n",
            "  \n",
            "[[1987.  576.]\n",
            " [ 510. 1991.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bG7oxFrHHO9"
      },
      "source": [
        "## section 5-2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7s3mItLE5Wf"
      },
      "source": [
        "class Naive_Bayes_Bigram_Classifier():\n",
        "    def __init__(self , f_set , delta = 0.5):\n",
        "        self.f_set = f_set\n",
        "        self.delta = delta\n",
        "        self.labels = ['moulavi','amir']\n",
        "    \n",
        "    def train (self , data):\n",
        "        self.data = data\n",
        "        self.vocab_size = len(list(set(self.data['all']['unigrams'])))\n",
        "        self.all_vocab = len(self.data['all']['unigrams'])\n",
        "        \n",
        "    def predict (self , beyts):\n",
        "        tok = []\n",
        "        tok += bigram_line(beyts[0])\n",
        "        tok += bigram_line(beyts[1])\n",
        "        self.pred_probs = {'moulavi' : 0 , 'amir' : 0 }\n",
        "        for l in self.labels:\n",
        "            prob = 1\n",
        "            c_l = len(self.data[l]['beyt']) / len(self.data['all']['beyt'])\n",
        "            B=0\n",
        "            for t in tok:\n",
        "                try :\n",
        "                    self.data[l]['bigram_count'][t]\n",
        "                    B+=1\n",
        "                except : pass\n",
        "            for t in tok:\n",
        "                token = unigram_line(t)\n",
        "                try: \n",
        "                    c_bi = self.data[l]['bigram_count'][t]\n",
        "                    c_token = self.data[l]['unigram_count'][token[0]]\n",
        "                    prob *= (c_bi - self.delta) / c_token\n",
        "\n",
        "                except : \n",
        "                    try : \n",
        "                        c_unigram = self.data[l]['unigram_count'][token[1]]\n",
        "                        c_token = 1\n",
        "                        prob *= (self.delta * B * c_unigram) / (c_token * self.all_vocab)\n",
        "                    except :\n",
        "                        prob *= 1 / self.vocab_size\n",
        "\n",
        "            self.pred_probs[l] = prob * c_l\n",
        "            \n",
        "        return self.pred_probs\n",
        "    \n",
        "    def score (self , test_d):\n",
        "        true_lable = 0\n",
        "        pred_list=[]\n",
        "        self.confusion_matrix = np.zeros([2,2])\n",
        "        self.measures = {'moulavi' : {'tp': 0 , 'fp' : 0 , 'fn' : 0  , 'Precision' : 0 , 'recall' : 0, 'f1' : 0}\n",
        "                         , 'amir' : {'tp': 0 , 'fp' : 0 , 'fn' : 0  , 'Precision' : 0 , 'recall' : 0, 'f1' : 0} }\n",
        "        for l in self.labels:\n",
        "            for b in test_d[l]['beyt']:\n",
        "                pred = self.predict(b)\n",
        "                pred = list(pred.values())\n",
        "                max_prob = self.labels[np.argmax(pred)]\n",
        "                pred_list.append(max_prob)\n",
        "                if max_prob == l : \n",
        "                    true_lable += 1\n",
        "                    self.measures[l]['tp'] +=1\n",
        "                    self.confusion_matrix[self.labels.index(l),self.labels.index(l)] += 1\n",
        "                else :\n",
        "                    self.measures[l]['fn'] +=1\n",
        "                    self.measures[max_prob]['fp'] +=1 \n",
        "                    self.confusion_matrix[self.labels.index(l),self.labels.index(max_prob)] += 1\n",
        "                    \n",
        "        self.acc = true_lable / len(test_d['all']['beyt'])\n",
        "        return self.acc , pred_list\n",
        "            \n",
        "    def cal_measures(self , avg = 'macro_avreging'):\n",
        "        for l in self.labels:\n",
        "            self.measures[l]['precision'] = self.measures[l]['tp'] / (self.measures[l]['tp'] \n",
        "                                                                             + self.measures[l]['fp'])\n",
        "            self.measures[l]['recall'] = self.measures[l]['tp'] / (self.measures[l]['tp'] \n",
        "                                                                             + self.measures[l]['fn'])\n",
        "            self.measures[l]['f1'] = (2*self.measures[l]['precision']*self.measures[l]['recall']) / (self.measures[l]['precision'] + self.measures[l]['recall'])\n",
        "        precision , recall = 0 , 0\n",
        "        for l in self.labels:\n",
        "            if avg == 'macro_avreging': c = 1\n",
        "            if avg == 'micro_avreging': c = len(self.data[l]['beyt']) / len(self.data['all']['beyt'])\n",
        "            precision += (c * self.measures[l]['precision'])\n",
        "            recall += (c * self.measures[l]['recall'])\n",
        "        precision /= len(self.labels)\n",
        "        recall /= len(self.labels)\n",
        "        f1 = (2 * precision * recall) / (precision + recall)\n",
        "        return precision, recall, f1\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQ8qItM4FE1I",
        "outputId": "57a10a5f-c6ee-4f0c-b9aa-cf501e3967da"
      },
      "source": [
        "train2_data = prepare2_data('/content/drive/MyDrive/nlp/HW1/train.txt' , ['moulavi' , 'amir'])\n",
        "test2_data = prepare2_data('/content/drive/MyDrive/nlp/HW1/test.txt' , ['moulavi' , 'amir'])\n",
        "\n",
        "bigrams_set= list(set(train2_data['all']['bigrams']))\n",
        "bnb = Naive_Bayes_Bigram_Classifier(bigrams_set , delta=0.6 )\n",
        "bnb.train(train2_data)\n",
        "acc , pred = bnb.score(test2_data)\n",
        "print('Accuracy = {:4.2f} % '.format(acc * 100))\n",
        "print('  ')\n",
        "print(bnb.confusion_matrix)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy = 91.85 % \n",
            "  \n",
            "[[2343.  220.]\n",
            " [ 145. 1770.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYySRG8kGt8F"
      },
      "source": [
        "## section 5-3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nei84OG8F4u9"
      },
      "source": [
        "class Naive_Bayes_Bigram_Classifier():\n",
        "    def __init__(self , f_set , delta = 0.5):\n",
        "        self.f_set = f_set\n",
        "        self.delta = delta\n",
        "        self.labels = ['bahar','ghaani']\n",
        "    \n",
        "    def train (self , data):\n",
        "        self.data = data\n",
        "        self.vocab_size = len(list(set(self.data['all']['unigrams'])))\n",
        "        self.all_vocab = len(self.data['all']['unigrams'])\n",
        "        \n",
        "    def predict (self , beyts):\n",
        "        tok = []\n",
        "        tok += bigram_line(beyts[0])\n",
        "        tok += bigram_line(beyts[1])\n",
        "        self.pred_probs = {'bahar' : 0 , 'ghaani' : 0 }\n",
        "        for l in self.labels:\n",
        "            prob = 1\n",
        "            c_l = len(self.data[l]['beyt']) / len(self.data['all']['beyt'])\n",
        "            B=0\n",
        "            for t in tok:\n",
        "                try :\n",
        "                    self.data[l]['bigram_count'][t]\n",
        "                    B+=1\n",
        "                except : pass\n",
        "            for t in tok:\n",
        "                token = unigram_line(t)\n",
        "                try: \n",
        "                    c_bi = self.data[l]['bigram_count'][t]\n",
        "                    c_token = self.data[l]['unigram_count'][token[0]]\n",
        "                    prob *= (c_bi - self.delta) / c_token\n",
        "\n",
        "                except : \n",
        "                    try : \n",
        "                        c_unigram = self.data[l]['unigram_count'][token[1]]\n",
        "                        c_token = 1\n",
        "                        prob *= (self.delta * B * c_unigram) / (c_token * self.all_vocab)\n",
        "                    except :\n",
        "                        prob *= 1 / self.vocab_size\n",
        "\n",
        "            self.pred_probs[l] = prob * c_l\n",
        "            \n",
        "        return self.pred_probs\n",
        "    \n",
        "    def score (self , test_d):\n",
        "        true_lable = 0\n",
        "        pred_list=[]\n",
        "        self.confusion_matrix = np.zeros([2,2])\n",
        "        self.measures = {'bahar' : {'tp': 0 , 'fp' : 0 , 'fn' : 0  , 'Precision' : 0 , 'recall' : 0, 'f1' : 0}\n",
        "                         , 'ghaani' : {'tp': 0 , 'fp' : 0 , 'fn' : 0  , 'Precision' : 0 , 'recall' : 0, 'f1' : 0} }\n",
        "        for l in self.labels:\n",
        "            for b in test_d[l]['beyt']:\n",
        "                pred = self.predict(b)\n",
        "                pred = list(pred.values())\n",
        "                max_prob = self.labels[np.argmax(pred)]\n",
        "                pred_list.append(max_prob)\n",
        "                if max_prob == l : \n",
        "                    true_lable += 1\n",
        "                    self.measures[l]['tp'] +=1\n",
        "                    self.confusion_matrix[self.labels.index(l),self.labels.index(l)] += 1\n",
        "                else :\n",
        "                    self.measures[l]['fn'] +=1\n",
        "                    self.measures[max_prob]['fp'] +=1 \n",
        "                    self.confusion_matrix[self.labels.index(l),self.labels.index(max_prob)] += 1\n",
        "                    \n",
        "        self.acc = true_lable / len(test_d['all']['beyt'])\n",
        "        return self.acc , pred_list\n",
        "            \n",
        "    def cal_measures(self , avg = 'macro_avreging'):\n",
        "        for l in self.labels:\n",
        "            self.measures[l]['precision'] = self.measures[l]['tp'] / (self.measures[l]['tp'] \n",
        "                                                                             + self.measures[l]['fp'])\n",
        "            self.measures[l]['recall'] = self.measures[l]['tp'] / (self.measures[l]['tp'] \n",
        "                                                                             + self.measures[l]['fn'])\n",
        "            self.measures[l]['f1'] = (2*self.measures[l]['precision']*self.measures[l]['recall']) / (self.measures[l]['precision'] + self.measures[l]['recall'])\n",
        "        precision , recall = 0 , 0\n",
        "        for l in self.labels:\n",
        "            if avg == 'macro_avreging': c = 1\n",
        "            if avg == 'micro_avreging': c = len(self.data[l]['beyt']) / len(self.data['all']['beyt'])\n",
        "            precision += (c * self.measures[l]['precision'])\n",
        "            recall += (c * self.measures[l]['recall'])\n",
        "        precision /= len(self.labels)\n",
        "        recall /= len(self.labels)\n",
        "        f1 = (2 * precision * recall) / (precision + recall)\n",
        "        return precision, recall, f1\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXuJ9cQFF8uC",
        "outputId": "c3ef0224-5ed2-4a3d-9db1-922068b6bb6f"
      },
      "source": [
        "train2_data = prepare2_data('/content/drive/MyDrive/nlp/HW1/train.txt' , ['bahar' , 'ghaani'])\n",
        "test2_data = prepare2_data('/content/drive/MyDrive/nlp/HW1/test.txt' , ['bahar' , 'ghaani'])\n",
        "\n",
        "bigrams_set= list(set(train2_data['all']['bigrams']))\n",
        "bnb = Naive_Bayes_Bigram_Classifier(bigrams_set , delta=0.6 )\n",
        "bnb.train(train2_data)\n",
        "acc , pred = bnb.score(test2_data)\n",
        "print('Accuracy = {:4.2f} % '.format(acc * 100))\n",
        "print('  ')\n",
        "print(bnb.confusion_matrix)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy = 79.76 % \n",
            "  \n",
            "[[1538.  566.]\n",
            " [ 277. 1784.]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}